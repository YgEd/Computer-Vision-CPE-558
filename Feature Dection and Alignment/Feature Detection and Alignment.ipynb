{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS 558 Homework - Due November 7th\n",
        "## Saoirse Mooney & Joshua Meharg\n",
        "I pledge my honor that I have abided by the Stevens Honor System.\n"
      ],
      "metadata": {
        "id": "JD8DAr2qQpFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the Harris corner detector for a gray value image. You should leverage \n",
        "the code developed so far in the course and compute the “cornerness” response \n",
        "function of each point based on the second moment matrix. The final feature \n",
        "selection will then pick the 1000 strongest points as the features of the image. \n",
        "Execute this detector on greyscale versions of the two images in the \n",
        "TwoViewAlignment  data subdirectory. Include the results in your report. "
      ],
      "metadata": {
        "id": "tMLq3mw-lnVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import statements"
      ],
      "metadata": {
        "id": "WNn27FkglppR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "wGOg1P3wZnLU"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline \n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import random as rand\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import copy \n",
        "from PIL import Image as im\n",
        "from IPython.display import Image\n",
        "import cv2\n",
        "from numpy.core.memmap import uint8\n",
        "import imutils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convery both images to grey scale and open side-by-side"
      ],
      "metadata": {
        "id": "xcOTymbmmTUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sidebyside(img1in, img2in):\n",
        "  \"\"\"\n",
        "  Summary: Take two images and creates a new image of them side-by-side\n",
        "  \"\"\"\n",
        "  #creates new image with img1in on the left and img2in on the right\n",
        "  print(\"img1 shape = \" + str(img1in.shape))\n",
        "  print(\"img2 shape = \" + str(img2in.shape))\n",
        "\n",
        "  img1 = copy.copy(img1in)\n",
        "  row1 = len(img1)\n",
        "  col1 = len(img1[0])\n",
        "\n",
        "  img2 = copy.copy(img2in)\n",
        "  row2 = len(img2)\n",
        "  col2 = len(img2[0])\n",
        "  #(rows, columns)\n",
        "  row_count = max(row1, row2)\n",
        "  result = np.zeros([row_count, col1+col2])\n",
        "  print(\"result shape = \" + str(result.shape))\n",
        "   #painting img1\n",
        "  for i in range(0, row1):\n",
        "    for j in range(0, col1):\n",
        "      result[i][j] = img1[i][j]\n",
        "\n",
        "  #painting img2\n",
        "  for i in range(0, row2):\n",
        "    for j in range(0, col2):\n",
        "      result[i][j+col1] = img2[i][j]\n",
        "\n",
        "  return result  "
      ],
      "metadata": {
        "id": "XbYTx_hHpYFV"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "QdnGdPUUPSsS"
      },
      "outputs": [],
      "source": [
        "def generateWindow(s):\n",
        "    X = np.zeros([s,s], dtype=int)\n",
        "    Y = np.zeros([s,s], dtype=int)\n",
        "    rnge = math.floor(s/2)\n",
        "    curr_r = -1*rnge\n",
        "    for i in range(0,s):\n",
        "        Y[i] = np.full(s, curr_r)        \n",
        "        for j in range(0,s):\n",
        "            X[j][i] = curr_r\n",
        "        curr_r += 1 \n",
        "    \n",
        "    return X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "kjNh1syIJPnu"
      },
      "outputs": [],
      "source": [
        "def generateGaussianFilter(sigma): \n",
        "    # generate windows\n",
        "    X,Y = generateWindow(6*sigma-1)\n",
        "\n",
        "    coef = 1 / (2 * math.pi * sigma**2)\n",
        "    s = len(X)\n",
        "    result = np.zeros([s, s])\n",
        "    \n",
        "    for i in range(0, s):\n",
        "        for j in range(0, s):\n",
        "            xi = X[i][j]\n",
        "            yi = Y[i][j]\n",
        "            power = -1 * (xi*xi + yi*yi) / (2*sigma*sigma)\n",
        "            result[i][j] = coef * math.exp(power)\n",
        "    \n",
        "    filter_sum = np.sum(np.sum(result))\n",
        "    print(\"The sum of the filter coefficients is \",filter_sum)\n",
        "    if filter_sum < 0.95:\n",
        "        print(\"WARNING: the filter coefficients do not sum to 1\")\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "87aB8WKlJYPJ"
      },
      "outputs": [],
      "source": [
        "def applyGaussian(filtr, input_img):\n",
        "    s = len(filtr)\n",
        "    rows = len(input_img)-(2*math.floor(s/2))\n",
        "    cols = len(input_img[0])-(2*math.floor(s/2))\n",
        "    result = np.zeros([rows, cols])\n",
        "    \n",
        "    for x in tqdm(range(0, rows)):\n",
        "        for y in range(0, cols):\n",
        "            local_pixels = np.zeros([s, s])\n",
        "            # fill temp matrix w elems from image\n",
        "            for i in range(0, s):\n",
        "                for j in range(0, s):\n",
        "                    local_pixels[i][j] = input_img[x+i][y+j]\n",
        "            # convolve filter and local pixels   \n",
        "            result[x][y]=np.sum(np.sum(np.multiply(filtr,local_pixels)))\n",
        "            #result[x][y] = np.sum(np.multiply(local_pixels.flatten(), np.flip(filtr.flatten())))\n",
        "        \n",
        "    return result             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "teP6ZUetMPks"
      },
      "outputs": [],
      "source": [
        "def convolve(filtr, img):\n",
        "    image_rows = len(img)\n",
        "    image_columns = len(img[0])\n",
        "    # create a copy of image for padding\n",
        "    s = len(filtr)\n",
        "    print(\"img size\",np.shape(img))\n",
        "\n",
        "    result = np.zeros([image_rows, image_columns])\n",
        "    \n",
        "    for x in tqdm(range(0,image_rows-2)):\n",
        "        for y in range(0,image_columns-2):\n",
        "            local_pixels = np.zeros([s, s])\n",
        "            # fill temp matrix w elems from image\n",
        "            for i in range(0, s):\n",
        "                for j in range(0, s):\n",
        "                    local_pixels[i][j] = img[x+i][y+j]\n",
        "            result[x][y]=np.sum(np.sum(np.multiply(filtr,local_pixels)))\n",
        "            #result[x][y]=np.sum(np.multiply(local_pixels.flatten(), np.flip(filtr.flatten())))\n",
        "                        \n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "khGNeeXIMT-R"
      },
      "outputs": [],
      "source": [
        "def gradient_computation(Gx, Gy):\n",
        "    rows = len(Gx)\n",
        "    columns = len(Gx[0])\n",
        "    result = np.zeros([rows, columns])\n",
        "    \n",
        "    for i in range(0, rows):\n",
        "        for j in range(0, columns):\n",
        "            gx = Gx[i][j]\n",
        "            gy = Gy[i][j]\n",
        "            G = math.sqrt((gx**2) + (gy**2))\n",
        "            result[i][j] = G\n",
        "    return result\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "jhxOFT3E6cSp"
      },
      "outputs": [],
      "source": [
        "def non_max_sup(imgin):\n",
        "  img = copy.copy(imgin)\n",
        "  row = len(img)\n",
        "  col = len(img[0])\n",
        "    \n",
        "  \n",
        "  for i in tqdm(range(0, row)):\n",
        "    #print(\"i = \" + str(i))\n",
        "    for j in range(0, col):\n",
        "      #on upper left edge\n",
        "      if (i == 0 and j==0):\n",
        "        if (img[i][j] <= img[i][j+1]\n",
        "            or img[i][j] <= img[i+1][j]\n",
        "            or img[i][j] <= img[i+1][j+1]):\n",
        "              img[i][j] = 0\n",
        "      \n",
        "      #on bottom left\n",
        "      elif (i == len(img)-1 and j==0):\n",
        "        if (img[i][j] <= img[i][j+1]\n",
        "            or img[i][j] <= img[i-1][j]\n",
        "            or img[i][j] <= img[i-1][j+1]):\n",
        "              img[i][j] = 0\n",
        "      \n",
        "      #on top right\n",
        "      elif (i == 0 and j==len(img[0])-1):\n",
        "        if (img[i][j] <= img[i][j-1]\n",
        "            or img[i][j] <= img[i+1][j]\n",
        "            or img[i][j] <= img[i+1][j-1]):\n",
        "              img[i][j] = 0\n",
        "\n",
        "      #on bottom right\n",
        "      elif (i== len(img)-1 and j == len(img[0])-1):\n",
        "        if (img[i][j] <= img[i][j-1]\n",
        "            or img[i][j] <= img[i-1][j]\n",
        "            or img[i][j] <= img[i-1][j-1]):\n",
        "              img[i][j] = 0\n",
        "\n",
        "      #on left edge\n",
        "      elif (j == 0):\n",
        "        if (img[i][j] <= img[i-1][j] \n",
        "            or img[i][j] <= img[i-1][j+1]\n",
        "            or img[i][j] <= img[i][j+1]\n",
        "            or img[i][j] <= img[i+1][j+1]\n",
        "            or img[i][j] <= img[i+1][j]):\n",
        "            #dont keep current point\n",
        "              img[i][j] = 0\n",
        "      \n",
        "      #on right edge\n",
        "      elif (j == len(img[0])-1):\n",
        "        if (img[i][j] <= img[i-1][j] #above\n",
        "            or img[i][j] <= img[i-1][j-1] #top left\n",
        "            or img[i][j] <= img[i][j-1] #left\n",
        "            or img[i][j] <= img[i+1][j-1] #bottom left\n",
        "            or img[i][j] <= img[i+1][j]):\n",
        "            #dont keep current point\n",
        "              img[i][j] = 0\n",
        "            \n",
        "      #on top edge\n",
        "      elif (i == 0):\n",
        "        if (img[i][j] <= img[i][j-1] #left\n",
        "            or img[i][j] <= img[i+1][j-1] #bottom left\n",
        "            or img[i][j] <= img[i+1][j] #below\n",
        "            or img[i][j] <= img[i+1][j+1] #bottom right\n",
        "            or img[i][j] <= img[i][j+1]):\n",
        "            #dont keep current point\n",
        "              img[i][j] = 0\n",
        "            \n",
        "      #on bottom edge\n",
        "      elif (i == len(img)-1):\n",
        "        if (img[i][j] <= img[i][j-1] #left\n",
        "            or img[i][j] <= img[i-1][j-1] #top left\n",
        "            or img[i][j] <= img[i-1][j] #above\n",
        "            or img[i][j] <= img[i-1][j+1] #top right\n",
        "            or img[i][j] <= img[i][j+1]):\n",
        "            #dont keep current point\n",
        "              img[i][j] = 0\n",
        "\n",
        "      # middle case\n",
        "      elif (img[i][j] <= img[i-1][j-1] \n",
        "          or img[i][j] <= img[i][j-1] \n",
        "          or img[i][j] <= img[i+1][j-1]\n",
        "          or img[i][j] <= img[i+1][j]\n",
        "          or img[i][j] <= img[i+1][j+1]\n",
        "          or img[i][j] <= img[i][j+1]\n",
        "          or img[i][j] <= img[i-1][j+1]\n",
        "          or img[i][j] <= img[i-1][j]):\n",
        "          #dont keep current point\n",
        "            img[i][j] = 0\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pick top 1000 feature points before applying non max suppression\n",
        "def top1000(imgin):\n",
        "  img = copy.copy(imgin)\n",
        "  lst = []\n",
        "  row = len(img)\n",
        "  col = len(img[0])\n",
        "  for i in tqdm(range (0, 1000)):\n",
        "    index = np.unravel_index(np.argmax(img), img.shape)\n",
        "    lst += [index]\n",
        "    img[index] = 0\n",
        "  return lst"
      ],
      "metadata": {
        "id": "5-YLf2XvYk78"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outputs an image result with shape of img with white pixels for coordinates in lst\n",
        "#if u specify white as true than paints with 255\n",
        "def paint(img, lst, white=False):\n",
        "  image_rows = len(img)\n",
        "  image_columns = len(img[0])\n",
        "  result = np.zeros([image_rows, image_columns])\n",
        "  for i in range(0, len(lst)):\n",
        "    if (white == False):\n",
        "      result[lst[i]] = img[lst[i]]\n",
        "    elif (white):\n",
        "      result[lst[i]] = 255\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "9F32lr8sanKK"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process(imgin):\n",
        "  \"\"\"\n",
        "  Summary: #main function for applying guassian, creating second moment matrix, taking determinate, \n",
        "  getting 1000 feature points and applying non-max suppression\n",
        "  \"\"\"\n",
        "\n",
        "  horizontal_sobel_filter = np.array([[1, 0, -1], [2, 0, -2], [1, 0, -1]])\n",
        "  vertical_sobel_filter = np.array([[1, 2, 1], [0, 0, 0], [-1, -2, -1]])\n",
        "\n",
        "  gaussian_filtrp = generateGaussianFilter(1)\n",
        "  gauss_img_matrixp = applyGaussian(gaussian_filtrp, imgin)\n",
        "\n",
        "  # gaussian already applied to input img\n",
        "  XXp = convolve(horizontal_sobel_filter, (convolve(horizontal_sobel_filter, gauss_img_matrixp))) #caluclate Ixx\n",
        "  XYp = convolve(vertical_sobel_filter, (convolve(horizontal_sobel_filter, gauss_img_matrixp))) #calculate Ixy\n",
        "  YYp = convolve(vertical_sobel_filter, (convolve(vertical_sobel_filter, gauss_img_matrixp))) #calculate Iyy\n",
        "\n",
        "  det_xx_yyp = np.multiply(XXp, YYp)\n",
        "  det_xy_xyp = np.multiply(-1,np.multiply(XYp,XYp))\n",
        "\n",
        "  #determinate\n",
        "  res_imgp = np.sum([det_xx_yyp, det_xy_xyp], axis=0)\n",
        "\n",
        "  \n",
        "  features = top1000(res_imgp)\n",
        "  features1 = paint(res_imgp, features)\n",
        "  features2 = non_max_sup(features1)\n",
        "\n",
        "  # fe1 = im.fromarray(features1.astype(uint8))\n",
        "  # fe1.save('initial_top1000_features.jpg')\n",
        "  # fe2 = im.fromarray(features2.astype(uint8))\n",
        "  # fe2.save('feature_points_afternon-max.jpg')\n",
        "\n",
        "  return features1, features2\n"
      ],
      "metadata": {
        "id": "f2qGQOGOtaJq"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def featurepoints(img):\n",
        "  #returns a list of tuples representing coordinates of feature points of img\n",
        "  lst = []\n",
        "  row = len(img)\n",
        "  col = len(img[0])\n",
        "  for i in tqdm(range(0, row)):\n",
        "    for j in range(0, col):\n",
        "      if (img[i][j] > 0):\n",
        "        lst += [(i, j)]\n",
        "  return lst"
      ],
      "metadata": {
        "id": "Cj-FMEhpGGK_"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def offset(l1, imgin):\n",
        "  out = l1\n",
        "  cols = len(imgin[0])\n",
        "  # offsets the col value inorder to print line correctly for side by side img\n",
        "  for i in range(0, len(l1)):\n",
        "    out[i] = (l1[i][0], l1[i][1] + cols)\n",
        "  return out"
      ],
      "metadata": {
        "id": "J8w1Ey6yLwzz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def secondmax(imgin):\n",
        "  img = copy.copy(imgin)\n",
        "  row = len(img)\n",
        "  col = len(img[0])\n",
        "  index = np.unravel_index(np.argmax(img), img.shape)\n",
        "  img[index] = 0\n",
        "  return "
      ],
      "metadata": {
        "id": "XqGcwyAC7JCe"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getmax(imgin):\n",
        "  return np.unravel_index(np.argmax(imgin), imgin.shape)"
      ],
      "metadata": {
        "id": "zDnL8xzj8vS-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def paintlines(imgin, c1, c2):\n",
        "  img = copy.copy(imgin)\n",
        "  # c1 is the feature points coords for the left image and c2 for the right\n",
        "  for i in range(0, len(c1)):\n",
        "    img = cv2.line(img, [c1[i][1], c1[i][0]], [c2[i][1], c2[i][0]], (255, 87, 51), 2) \n",
        "    # img = cv2.circle(img, [c1[i][1], c1[i][0]], 6, (255, 0, 0), 1)\n",
        "    # img = cv2.circle(img, [c2[i][1], c2[i][0]], 6, (255, 0, 0), 1)\n",
        "    # break\n",
        "  return img"
      ],
      "metadata": {
        "id": "iL17tU4QNE3o"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getbox(coord, n, img):\n",
        "    #coord in format (x, y)\n",
        "    #returns list of window of values with coord at center that is\n",
        "    #3^n by 3^n\n",
        "    out = []\n",
        "    topleftcorner = coord\n",
        "    for i in range(0, n):\n",
        "        topleftcorner = (topleftcorner[0]-1, topleftcorner[1]-1)\n",
        "    \n",
        "    curr = topleftcorner\n",
        "    for i in range(0, 2*n+1):\n",
        "        for j in range(0,  2*n+1):\n",
        "            out += [img[curr[0]+i][curr[1]+j]]\n",
        "    return out"
      ],
      "metadata": {
        "id": "ctX9cD5q2ggc"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def NCCpoint(list1, list2, img1, img2, n):\n",
        "  # does NCC with n x n area window\n",
        "  if (n%2==0 or n==1):\n",
        "    print(\"error, n has to be odd and greater than 1\")\n",
        "  n = int((n)/2)\n",
        "  out = []\n",
        "  # Takes in feature points of img2 and img2 which is stored as list2 and list2 and calculates the SSD distance between two sets of feature points \n",
        "  for i in (tqdm(range(0, len(list1)))):\n",
        "    for j in range(0, len(list2)):\n",
        "        # window around the center point for feature\n",
        "        # point from img2\n",
        "        center1 = img1[list1[i]]\n",
        "        box1 = getbox(list1[i], n, img1)\n",
        "        mean1 = (sum(box1)) / pow(2*n+1, 2)\n",
        "\n",
        "        # window around the center point for feature\n",
        "        # point from img2\n",
        "        \n",
        "        center2 = img2[list2[j]]\n",
        "        box2 = getbox(list2[j], n, img2)\n",
        "        mean2 = (sum(box2)) / pow(2*n+1, 2)\n",
        "\n",
        "        # I1(x) - I1mean * I2(x) - I2mean\n",
        "        top = 0\n",
        "        for t in range(0, len(box1)):\n",
        "            top += (box1[t] - mean1)*(box2[t] - mean2)\n",
        "        \n",
        "        #(I1(x) - I1mean)^2\n",
        "        botl = 0\n",
        "        for t in range(0, len(box1)):\n",
        "            botl += pow((box1[t]-mean1),2)\n",
        "        \n",
        "        #(I2(x) - I2mean)^2\n",
        "        botr = 0\n",
        "        for t in range(0, len(box1)):\n",
        "            botr += pow((box2[t]-mean2),2)\n",
        "\n",
        "        currans = top / math.sqrt((botr*botl))\n",
        "\n",
        "        out += [(list1[i], list2[j], currans)]\n",
        "  return out"
      ],
      "metadata": {
        "id": "9NgX-xaW2irB"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def max20(listin):\n",
        "  copy = listin\n",
        "  out1 = []\n",
        "  out2 = []\n",
        "  max = 0\n",
        "  for x in tqdm(range(0, 20)):\n",
        "    for i in range(0, len(listin)):\n",
        "      if (copy[i][2] > max):\n",
        "        max = copy[i][2]\n",
        "        max_in = i\n",
        "\n",
        "    out1 += [listin[max_in][0]]\n",
        "    out2 += [listin[max_in][1]]\n",
        "    copy[max_in] = (copy[max_in][0], copy[max_in][1], -1)\n",
        "    max = 0\n",
        "  return out1, out2\n"
      ],
      "metadata": {
        "id": "5Qgc1tt553ha"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion:\n",
        "SSD takes a template image and iterates over its discrete pixels to find where those pixels are located in the corresponding image. It does this by taking the sum of squared differences by calculating the residual error at every displacement in the test image. it assumes taht corresponding pixel values between the template and the test are the same. NNC on the other hand find displacements where the cross-correlation between the images is maximized. NNC works better for images with different exposures and the range will always be [-1,1].\n",
        "\n",
        "In our test, NCC worked well in matching similar points which are mainly in the middle of the side by side image. There appear to be around 2 outliers, but the bulk of the points, the other 18, seem to match well\n",
        "\n",
        "## Extra Credit\n",
        "When we rotate one of the images by 45 degrees as you can see in image problem1_extracredit the features match less efficiently. This is we use the windows around and including each feature point to compare and if one image has rotated, the values in the window will change even if the central pixels are the same. This can be solved by matching the rotations of the two images/correcting the planes of the image so that they are equivalent to properly extract the equivalent windows\n"
      ],
      "metadata": {
        "id": "S_g4bzUJUCFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def problem1_main(img1, img2, rotate=False, rotate_img=0,rotate_deg=0):\n",
        "  \"\"\"\n",
        "  Summary: Takes in two images and rotation wants, and performs corner detection, non-maxima suppression, and feature detection to compute the SSD od the images.\n",
        "  Parameters:\n",
        "  img1 (str): jpg\n",
        "  img2 (Str): jpg\n",
        "  rotate (bool): True if you want one of the images to be rotates\n",
        "  rotate_img (int): Specify img1 as 1 or img2 as 2\n",
        "  rotate_deg (int): degree to rotate specified image\n",
        "\n",
        "  Return:\n",
        "  Image of correspondances\n",
        "  \"\"\"\n",
        "  # open images and convert to grey scale\n",
        "  im1 = im.open(img1).convert(\"L\")\n",
        "  im1 = np.array(im1)\n",
        "  im2 = im.open(img2).convert(\"L\")\n",
        "  im2 = np.array(im2)\n",
        "\n",
        "  if rotate==True:\n",
        "    if rotate_img == 1:\n",
        "      im1 = imutils.rotate_bound(im1,rotate_deg)\n",
        "    elif rotate_img == 2:\n",
        "      im2 = imutils.rotate_bound(im2,rotate_deg)\n",
        "    else:\n",
        "      print(\"Error: rotate_img not valid\")\n",
        "      return\n",
        "  c_1, p_im1 = process(im1)\n",
        "  c_2, p_im2 = process(im2)\n",
        "\n",
        "  if rotate!=True:\n",
        "    c_1 = im.fromarray(c_1.astype(uint8))\n",
        "    c_1.save('uttowerleft_top100features.jpg')\n",
        "\n",
        "    c_2 = im.fromarray(c_2.astype(uint8))\n",
        "    c_2.save('uttowerright_top100features.jpg')\n",
        "\n",
        "    pp_im2 = im.fromarray(p_im2.astype(uint8))\n",
        "    pp_im2.save('uttowerright_features_after_non-max.jpg')\n",
        "\n",
        "    pp_im1 = im.fromarray(p_im1.astype(uint8))\n",
        "    pp_im1.save('uttowerleft_features_after_non-max.jpg')\n",
        "  else:\n",
        "    c_1 = im.fromarray(c_1.astype(uint8))\n",
        "    c_1.save('uttowerleft_top100features_rotated.jpg')\n",
        "\n",
        "    c_2 = im.fromarray(c_2.astype(uint8))\n",
        "    c_2.save('uttowerright_top100features.jpg')\n",
        "\n",
        "    pp_im2 = im.fromarray(p_im2.astype(uint8))\n",
        "    pp_im2.save('uttowerright_features_after_non-max.jpg')\n",
        "\n",
        "    pp_im1 = im.fromarray(p_im1.astype(uint8))\n",
        "    pp_im1.save('uttowerleft_features_after_non-max_rotated.jpg')\n",
        "\n",
        "\n",
        "  fp1 = featurepoints(p_im1)\n",
        "  fp2 = featurepoints(p_im2)\n",
        "  NCC = NCCpoint(fp1, fp2, im1, im2, 9)\n",
        "  points1, points2 = max20(NCC)\n",
        "  img_canvas = sidebyside(im1, im2)\n",
        "  ans_img = paintlines(img_canvas, points1, offset(points2, im2))\n",
        "  ans_img = im.fromarray(ans_img.astype(np.uint8))\n",
        "  if rotate != True:\n",
        "    ans_img.save('problem1_ans.jpg')\n",
        "  else:\n",
        "    ans_img.save('problem1_extracredit.jpg')\n",
        "\n",
        "  return Image('problem1_ans.jpg'), points1, points2, NCC\n"
      ],
      "metadata": {
        "id": "RJ0cZAnojyOW"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t, points1, points2, ohno = problem1_main('uttower_left.jpg', 'uttower_right.jpg')\n",
        "#ta, points1a, points2a, dontworry = problem1_main('uttower_left.jpg', 'uttower_right.jpg', True, 1, 45)"
      ],
      "metadata": {
        "id": "aNs2lW8YIaKz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e909b80-f355-4517-a899-2841d6614052"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of the filter coefficients is  0.9818147610543744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 679/679 [00:23<00:00, 29.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 42.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.79it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1524.88it/s]\n",
            "100%|██████████| 679/679 [00:01<00:00, 535.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sum of the filter coefficients is  0.9818147610543744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 679/679 [00:23<00:00, 28.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 42.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 42.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 43.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img size (679, 1020)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 677/677 [00:15<00:00, 42.88it/s]\n",
            "100%|██████████| 1000/1000 [00:00<00:00, 1653.45it/s]\n",
            "100%|██████████| 679/679 [00:01<00:00, 538.39it/s]\n",
            "100%|██████████| 679/679 [00:00<00:00, 1996.27it/s]\n",
            "100%|██████████| 679/679 [00:00<00:00, 2009.71it/s]\n",
            "100%|██████████| 379/379 [02:36<00:00,  2.42it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 54.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img1 shape = (683, 1024)\n",
            "img2 shape = (683, 1024)\n",
            "result shape = (683, 2048)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matching_points = []\n",
        "im1 = im.open('uttower_left.jpg').convert(\"L\")\n",
        "im1 = np.array(im1)\n",
        "points3 = points2\n",
        "\n",
        "for i in range(0, len(points3)):\n",
        "  points3[i] = (points3[i][0], points3[i][1] - len(im1[0]))\n",
        "\n",
        "for i in range(0, len(points1)):\n",
        "  matching_points += [(points1[i], points3[i])]\n",
        "matching_points"
      ],
      "metadata": {
        "id": "pc85XVKdc0f-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7329b13-eed4-4f17-bd7d-b099942860a2"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((549, 901), (518, 439)),\n",
              " ((557, 779), (524, 322)),\n",
              " ((535, 775), (502, 319)),\n",
              " ((495, 694), (460, 242)),\n",
              " ((371, 894), (347, 444)),\n",
              " ((552, 600), (516, 173)),\n",
              " ((371, 894), (357, 531)),\n",
              " ((552, 592), (515, 148)),\n",
              " ((552, 600), (515, 122)),\n",
              " ((579, 62), (524, 322)),\n",
              " ((552, 600), (515, 148)),\n",
              " ((552, 584), (516, 173)),\n",
              " ((552, 584), (515, 148)),\n",
              " ((552, 608), (515, 156)),\n",
              " ((552, 600), (515, 156)),\n",
              " ((552, 592), (516, 173)),\n",
              " ((513, 444), (353, 489)),\n",
              " ((541, 581), (504, 153)),\n",
              " ((553, 624), (516, 173)),\n",
              " ((552, 592), (515, 122))]"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Two View Image Alignment\n",
        "\n",
        "Using the feature detection code in the problem 1, build a set of putative \n",
        "feature correspondences using the following rules: a1) Selected the top 20 \n",
        "features based on the similarity measure score; a2) Select 30 random \n",
        "correspondences"
      ],
      "metadata": {
        "id": "uRIKNAwCbQS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 20 features as used in problem 1\n",
        "print(matching_points)"
      ],
      "metadata": {
        "id": "ZAZeq_kZbdMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8927ffba-8b95-41dd-9322-eae92bf5afb7"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((549, 901), (518, 439)), ((557, 779), (524, 322)), ((535, 775), (502, 319)), ((495, 694), (460, 242)), ((371, 894), (347, 444)), ((552, 600), (516, 173)), ((371, 894), (357, 531)), ((552, 592), (515, 148)), ((552, 600), (515, 122)), ((579, 62), (524, 322)), ((552, 600), (515, 148)), ((552, 584), (516, 173)), ((552, 584), (515, 148)), ((552, 608), (515, 156)), ((552, 600), (515, 156)), ((552, 592), (516, 173)), ((513, 444), (353, 489)), ((541, 581), (504, 153)), ((553, 624), (516, 173)), ((552, 592), (515, 122))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 random correspondences\n",
        "rand_t = rand.sample(ohno,30)\n",
        "rand_1 = []\n",
        "rand_2 = []\n",
        "rand_l = []\n",
        "for i in range(0, len(rand_t)):\n",
        "  rand_1.append(rand_t[i][0])\n",
        "  rand_2.append(rand_t[i][1])\n",
        "  rand_l.append((rand_t[i][0],rand_t[i][1]))\n"
      ],
      "metadata": {
        "id": "gDsS3AxpYhmL"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a RANSAC-based method to estimate an affine transformation \n",
        "between uttower_left.jpg and uttower_right.jpg in the TwoViewAlignment \n",
        "directory. First, run using as input only the putative correspondences defined in a1. Second use the aggregate putative correspondences of BOTH a1 AND \n",
        "a2. Show the inlier matches determined by RANSAC procedure (with \n",
        "adaptive iteration number) and compute their average feature reprojection \n",
        "error between the images.  Discuss what is the expected number of RANSAC \n",
        "iterations for each experiment and what is the actual number observed in \n",
        "practice. Include the results in your report. "
      ],
      "metadata": {
        "id": "FOTj42_Gbj8F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion: \n",
        "RANSAC will remove the outlier correspondances and leave only the inliners as long as the inliners outnumber the outliners\n",
        "\n",
        "The expected number of RANSAC iterations must be enough to ensure that four correct correspondances are selected from the list of all correspondances. If we are selecting the top 20 most likely correspondances, then the number of iterations will be much lower to reach an ideal homography. With a random selection of 30 correspondances, it is possible that the number of correct matches is less than the number of outliers, which would render the hoography unsolvable.\n",
        "\n",
        "In our code, neither selection of points works to generate a homography that causes the matches matrix multiplied by the homography to be close to 0, indicating a good model. This is likely due to an incorrect implementation of RANSAC for an affine transformation."
      ],
      "metadata": {
        "id": "2FOErmR-SwXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def affine_ransac(coor, N, e):\n",
        "  \"\"\"\n",
        "  Summary: randomly samples 4 coorepondances and builds a homography. repeats N times.\n",
        "  Parameters:\n",
        "  coor (list(((int, int)), ((int, int))): coordinates and value\n",
        "  N (int): number of iterations\n",
        "  e (int): accepted error\n",
        "  Returns:\n",
        "  model (int, array): number of inliners and the corresponding homography\n",
        "  \"\"\"\n",
        "  # randomly sample s correspondances -- s=4\n",
        "  # hypothesize the model h -- find the smallest eigen of At * a\n",
        "  # plug each point from the correspondances into the model and see if the error is small enough\n",
        "    # plug in one corr to the model and see if you get the correct matching coor -- minimize the error between the output and the correct coor\n",
        "  #coor = [((xs, ys),vs), ((xd,yd), vd)]\n",
        "  models = []\n",
        "  while N>0:\n",
        "    num_correspondances = len(coor)\n",
        "    rand_is = np.random.choice(a=num_correspondances, size=4, replace=False) # random indicies -- get 4 samples\n",
        "    s0 = coor[rand_is[0]]\n",
        "    c0_s = s0[0]\n",
        "    c0_d = s0[1]\n",
        "    s1 = coor[rand_is[1]]\n",
        "    c1_s = s1[0]\n",
        "    c1_d = s1[1]\n",
        "    s2 = coor[rand_is[2]]\n",
        "    c2_s = s2[0]\n",
        "    c2_d = s2[1]\n",
        "    s3 = coor[rand_is[3]]\n",
        "    c3_s = s3[0]\n",
        "    c3_d = s3[1]\n",
        "\n",
        "    s_arr = [s0,s1,s2,s3]  \n",
        "    #print(s_arr)  \n",
        "\n",
        "    # construct A matrix with 8 degrees of freedom\n",
        "    A = np.zeros((8,9))\n",
        "    A[0] = [c0_s[0], c0_s[1],1, 0,0,0, -1*c0_d[0]*c0_s[0], -1*c0_d[0]*c0_s[1], -1*c0_d[0]]\n",
        "    A[1] = [0,0,0, c0_s[0],c0_s[1],1, -1*c0_d[1]*c0_s[0],-1*c0_d[1]*c0_s[1],-1*c0_d[1]]\n",
        "\n",
        "    A[2] =  [c1_s[0], c1_s[1],1, 0,0,0, -1*c1_d[0]*c1_s[0], -1*c1_d[0]*c1_s[1], -1*c1_d[0]]\n",
        "    A[3] = [0,0,0, c1_s[0],c1_s[1],1, -1*c1_d[1]*c1_s[0],-1*c1_d[1]*c1_s[1],-1*c1_d[1]]\n",
        "\n",
        "    A[4] = [c2_s[0], c2_s[1],1, 0,0,0, -1*c2_d[0]*c2_s[0], -1*c2_d[0]*c2_s[1], -1*c2_d[0]]\n",
        "    A[5] = [0,0,0, c2_s[0],c2_s[1],1, -1*c2_d[1]*c2_s[0],-1*c2_d[1]*c2_s[1],-1*c2_d[1]]\n",
        "\n",
        "    A[6] =  [c3_s[0], c3_s[1],1, 0,0,0, -1*c3_d[0]*c3_s[0], -1*c3_d[0]*c3_s[1], -1*c3_d[0]]\n",
        "    A[7] = [0,0,0, c3_s[0],c3_s[1],1, -1*c3_d[1]*c3_s[0],-1*c3_d[1]*c3_s[1],-1*c3_d[1]]\n",
        "\n",
        "    #print(\"A\",A)\n",
        "    A_t = np.transpose(A)\n",
        "    #print(\"At\", A_t)\n",
        "    eig_val, eig_vec = np.linalg.eig(np.matmul(A_t,A))\n",
        "    lam_i = np.argmin(eig_val)\n",
        "    lam = eig_val[lam_i]\n",
        "    h = eig_vec[lam_i]\n",
        "    h = h.reshape((3,3)) # eigvec h with smallest eigval minimizes the loss function\n",
        "\n",
        "    # have h as out hypothesized homography -- now test\n",
        "    \n",
        "    #h, m = cv2.findHomography(np.array(p1), np.array(p2), cv2.RANSAC)\n",
        "    for c in range(0, num_correspondances):\n",
        "      inliners = np.array([])\n",
        "      xs = coor[c][0][0]\n",
        "      ys = coor[c][0][1]\n",
        "      xd = coor[c][1][0]\n",
        "      yd = coor[c][1][1]\n",
        "      src = np.array([[xs],[ys],[1]])\n",
        "      dst = np.matmul(h, src)\n",
        "      h_xd = dst[0] / dst[2]\n",
        "      h_yd = dst[1] / dst[2]\n",
        "      if abs(h_xd - xd)<=e and abs(h_yd - yd)<=e:\n",
        "        inliners = np.append(inliners, coor[c])\n",
        "\n",
        "    models.append((len(inliners), h))\n",
        "\n",
        "    N-=1\n",
        "\n",
        "  # return model w largest number of inliners\n",
        "  models.sort(key=lambda tup: tup[0])\n",
        "  return models[-1]\n"
      ],
      "metadata": {
        "id": "wqvz1hbLy0Lu"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h = affine_ransac(matching_points, 20, 100)\n",
        "h"
      ],
      "metadata": {
        "id": "hr-8fTh-5HkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2502e45-042b-498e-92cd-efbde3f307f6"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, array([[-1.15018352e-03, -1.11516771e-03, -4.87378677e-04],\n",
              "        [-6.48710661e-02,  6.56142576e-01, -7.45567213e-01],\n",
              "        [ 9.68085862e-02, -4.26652917e-03,  2.31579056e-03]]))"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand_h = affine_ransac(rand_l, 50, 100)"
      ],
      "metadata": {
        "id": "RGzM5qHtcMb6"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warp one image onto the other using the affine transformation estimates. To \n",
        "do this, you will need to learn about MATLAB maketform and imtransform \n",
        "functions (or their python/C counterparts). Create a new image big enough \n",
        "to hold the panorama and composite the two images into it. You may \n",
        "composite by simply averaging the pixel values where the two images \n",
        "overlap. Include the results in your report. "
      ],
      "metadata": {
        "id": "O9B_igaubwMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def warp(img1,img2, h):\n",
        "  \"\"\"\n",
        "  Summary: Warps one image onto another using affine transformation estimates\n",
        "  \"\"\"\n",
        "  h1,w1 = img1.shape\n",
        "  pts = np.float32([[0,0], [0,h1-1], [w1-1,h1-1], [w1-1,0]])\n",
        "  dst = cv2.perspectiveTransform(pts, h)\n",
        "  res = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)\n",
        "  return res"
      ],
      "metadata": {
        "id": "1pZ4T5vpTYrx"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because our homography had errors we can stitch the images together using the builtin cv2 functions"
      ],
      "metadata": {
        "id": "0KQKdUCxgs2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "paths = ['uttower_left.jpg','uttower_right.jpg']\n",
        "imgs = []\n",
        "for image in paths:\n",
        "    img = cv2.imread(image)\n",
        "    imgs.append(img)\n",
        "\n",
        "stitcher = cv2.Stitcher_create()\n",
        "err, stitched = stitcher.stitch(imgs)\n",
        "if (not err == True):\n",
        "    cv2.imwrite(\"stitched_img.jpg\", stitched)\n"
      ],
      "metadata": {
        "id": "XVT9plNBfGUx"
      },
      "execution_count": 139,
      "outputs": []
    }
  ]
}